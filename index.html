<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Anshul Singh</title>
  <meta name="author" content="Anshul Singh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/tornot.png">
  <style>
    html { scroll-behavior: smooth; }

    body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      background: #fafafa;
      color: #222;
    }

    table {
      width: 100%;
      max-width: 900px;
      margin: 0 auto;
      border: 0;
      border-spacing: 0;
    }

    td {
      padding: 0;
    }

    name {
      font-size: 2.2rem;
      font-weight: 700;
    }

    a {
      color: #0057c3;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    h2 {
      margin-top: 40px;
      margin-bottom: 20px;
      font-size: 1.8rem;
      text-align: center;
    }

    /* --- Reflections (Projects) --- */
    .section-heading {
      font-size: 2rem;
      margin-bottom: 30px;
      color: #111;
    }

    .project-container {
      max-width: 900px;
      margin: 0 auto;
      padding: 0 20px;
    }

    .project-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 25px;
    }

    .project-card {
      background: #fff;
      border: 1px solid #e0e0e0;
      border-radius: 12px;
      padding: 25px 20px;
      text-align: center;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }

    .project-card:hover {
      transform: translateY(-6px);
      box-shadow: 0 8px 20px rgba(0, 0, 0, 0.08);
    }

    .project-logo {
      width: 100%;
      height: 140px;
      object-fit: contain;
      margin-bottom: 15px;
    }

    .project-title {
      font-size: 1.2rem;
      font-weight: 600;
      margin-bottom: 10px;
    }

    .project-links a {
      font-size: 1rem;
      color: #0057c3;
      font-weight: 500;
    }

    /* --- Footer --- */
    #footer {
      margin-top: 60px;
      padding: 30px 0;
      text-align: center;
      font-size: 0.9rem;
      color: #555;
    }

    #clustrmaps-widget {
      margin: 10px auto 20px;
      max-width: 300px;
    }

    /* --- Responsive tweaks --- */
    @media (max-width: 768px) {
      name {
        font-size: 1.8rem;
      }
      .project-grid {
        grid-template-columns: 1fr;
      }
      #footer {
        padding: 20px 10px;
      }
    }
  </style>
</head>

<body>
  <table>
    <tbody>
      <tr>
        <td>
          <table>
            <tbody>
              <tr>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center"><name>Anshul Singh</name></p>
                  <p>I am a <strong>Research Associate at the <a href="https://www.iisc.ac.in/">IACV Lab, IISc Bangalore</a></strong>, advised by <strong><a href="https://scholar.google.co.in/citations?user=Rscf2pYAAAAJ&hl=en">Prof. Soma Biswas</a></strong>. My research centers on Large Language Models, with a particular focus on multimodal systems, visual reasoning, and document understanding. My work explores how to enhance the reasoning capabilities of language and vision models. Specifically, I have worked on:</p>
                  <ul>
                    <li>Multimodal Analysis of open source information using LLM-guided <strong>Active Learning</strong></li>
                    <li>Multimodal reasoning for multi-tabular data using Vision-Language Models</li>
                    <li><strong>Multilingual alignment</strong> for low-resource Indic languages</li>
                    <li>Vision Transformers and Signal analysis for fault detection and vocal pattern recognition</li>
                  </ul>
                  <p>Previously, I was a research intern at the <strong>LT Research Group, <a href="https://www.uni-hamburg.de/">University of Hamburg</a></strong>, with <a href="https://www.inf.uni-hamburg.de/en/inst/ab/lt/people/chris-biemann.html">Prof. Chris Biemann</a> and Jan Strich, and a <strong><a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">MITACS Globalink Research Intern</a></strong> at <a href="https://www.dal.ca/">Dalhousie University</a>. I completed my Bachelor of Engineering in Information Technology at <a href="https://puchd.ac.in/">Panjab University</a>. During my undergraduate studies, I also worked as a machine learning research intern at <a href="https://www.iitr.ac.in/">IIT Roorkee</a> and served as the ML/AI Lead at Google Developer Student Clubs.</p>
                  <p><strong>Research Interests:</strong> Multimodal and Multilingual Reasoning | Vision-Language Models | Interpretability of LLMs | Information Retrieval</p>
                  <p><strong>Hobbies:</strong> Outside of research, I enjoy traveling, reading, writing blogs, minimalist living, and tinkering with small experiments. I'm always happy to connect for a discussion or collaboration!</p>
                  <p style="text-align:center">
                    [<a href="mailto:anshulsinghchambial@gmail.com">Email</a> / 
                    <a href="data/my_CV.pdf">CV</a> / 
                    <a href="https://github.com/anshulsc">Github</a> / 
                    <a href="https://twitter.com/anshulsc">Twitter</a> / 
                    <a href="https://scholar.google.com/citations?hl=en&user=5vI_nOwAAAAJ">Google Scholar</a> / 
                    <a href="https://www.linkedin.com/in/anshulsc/">Linkedin</a> / 
                    <a href="https://blogs.anshulsc.live/">Blogs</a>]
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/prof.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/prof.png"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <h2>Experience & Education</h2>
          <table class="about-edu" width="100%">
            <tr>
              <td align="center" width="14%"><a href="https://iisc.ac.in/"><img src="images/iisc.jpg" alt="IISc" width="50%"></a></td>
              <td align="center" width="14%"><a href="https://www.uni-hamburg.de/en.html"><img src="images/UHH.svg" alt="UHamburg" width="50%"></a></td>
              <td align="center" width="14%"><a href="https://www.dal.ca/"><img src="images/dal.jpeg" alt="Dalhousie" width="50%"></a></td>
            </tr>

            <tr>
              <td align="center" style="padding-bottom: 40px;">Research Associate<br><b>IISc, Bangalore</b><br>Aug 2025 – Present</td>
              <td align="center" style="padding-bottom: 40px;">Research Intern<br><b>LT Group, UHH</b><br>Jan 2025 – May 2025</td>
              <td align="center" style="padding-bottom: 40px;">Undergrad Researcher<br><b>Dalhousie University, Halifax</b><br>Oct 2024 - April 2025</td>
            </tr>

            <tr>
              <td align="center" width="14%"><a href="https://www.mitacs.ca/"><img src="images/mitacs.jpeg" alt="mitacs" width="50%"></a></td>
              <td align="center" width="14%"><a href="https://www.iitr.ac.in/"><img src="images/IITR.svg" alt="IITR" width="50%"></a></td>
              <td align="center" width="14%"><a href="https://puchd.ac.in/"><img src="images/panjab.png" alt="PU" width="50%"></a></td>
            </tr>

            <tr>
              <td align="center">Research Intern<br><b>MITACS, Canada</b><br>June 2024 – Sep 2024</td>
              <td align="center">Machine Learning Research Intern<br><b>IIT, Roorkee</b><br>June 2023 – July 2023</td>
              <td align="center">B.E Information Technology<br><b>Panjab University</b><br>Sep 2021 – June 2025</td>
            </tr>
          </table>


          <h2>News</h2>
          <div class="scrollable-news" style="max-height:400px;overflow-y:auto;padding-right:8px;">
            <ul class="timeline" style="list-style:none;padding:0;margin:0;display:flex;flex-direction:column;gap:10px;">
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>Aug 2025</b>:</div><div><b>Paper</b> - Our paper <a href="https://arxiv.org/abs/2506.11684">MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space</a> is accepted at EMNLP 2025 (Findings).</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>Aug 2025</b>:</div><div><b>Position</b> - Started working as Research Associate at <a href="https://sites.google.com/iisc.ac.in/somabiswas/iacv-lab-iisc">IACV Lab</a>, IISc Bangalore.</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>May 2025</b>:</div><div><b>Paper</b> - Our new preprint, <a href="https://arxiv.org/abs/2506.11684">MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space</a>, is now available on ArXiv.</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>Jan 2025</b>:</div><div><b>Position</b> - Started a research internship at <a href="https://www.inf.uni-hamburg.de/en/inst/ab/lt/home.html">Language Technology Group</a>, University of Hamburg, Germany.</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>Oct 2024</b>:</div><div><b>Position</b> - Started working as undergraduate researcher at <a href="https://web.cs.dal.ca/~tushar/smart/">SMART Lab</a>, Dalhousie University.</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>July 2024</b>:</div><div><b>Position</b> - Selected for <a href="https://www.mitacs.ca/our-programs/globalink-research-internship-students/">Mitacs Globalink Research Internship</a> at Dalhousie University, Nova Scotia, Canada.</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>Nov 2023</b>:</div><div><b>Paper</b> - Our work <a href="https://ieeexplore.ieee.org/document/10306428">Comparative Analysis of State-of-the-Art Attack Detection Models</a> was accepted at 14th International Conference on Computing Communication and Networking Technologies (ICCCNT).</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>June 2023</b>:</div><div><b>Position</b> - Started a Machine Learning Research Intern at <a href="https://www.iitr.ac.in/">Virtual Labs, IIT Roorkee</a>.</div></li>
              <li style="display:flex;gap:12px;"><div style="white-space:nowrap;"><b>March 2022</b>:</div><div><b>Position</b> - Started working as Project Intern at Design & Innovation Centre, Panjab University.</div></li>
            </ul>
          </div>

          <h2>Research</h2>
          <table style="width:100%;border:0;border-spacing:0;">
            <tbody>
              <tr bgcolor="#CBC3E3">
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/mtabvqa.png" alt="mtabvqa" width="100%"></td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://supernova-event.ai/"><strong>MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space</strong></a><br>
                  <strong>Anshul Singh</strong>, <a href="https://www.inf.uni-hamburg.de/en/inst/ab/lt/people/chris-biemann.html">Chris Biemann</a>, Jan Strich<br>
                  <em>Empirical Methods of Natural Language Processing (EMNLP), 2025 Findings</em><br> 
                  <a href="https://arxiv.org/abs/2506.11684">Paper</a> / <a href="https://huggingface.co/mtabvqa">Dataset</a> / <a href="data/poster_mtabqa_emnlp.pdf">Poster</a>
                  <p>In this work, we address a critical gap in Vision-Language Model (VLM) evaluation by introducing MTabVQA, a novel benchmark for multi-tabular visual question answering. Our benchmark comprises 3,745 complex question-answer pairs that require multi-hop reasoning across several visually rendered table images, simulating real-world documents. We benchmark state-of-the-art VLMs, revealing significant limitations in their ability to reason over complex visual data. To address this, we release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments demonstrate that fine-tuning with our dataset substantially improves VLM performance, bridging the gap between existing benchmarks that rely on single or non-visual tables.</p>
                </td>
              </tr>
              <tr bgcolor="#CBC3E3">
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cot.jpeg" alt="IoT Intrusion Detection Analysis" width="100%"></td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://doi.org/10.1109/ICCCNT56998.2023.10306428"><strong>Comparative Analysis of State-of-the-Art Attack Detection Models</strong></a><br>
                  Priyanka Kumari, Veenu Mangat, and <strong>Anshul Singh</strong><br>
                  <em>14th International Conference on Computing Communication and Networking Technologies (ICCCNT), 2023</em><br>
                  <a href="https://doi.org/10.1109/ICCCNT56998.2023.10306428">Paper</a>
                  <p>In this work, we address the growing security challenges in IoT networks by conducting a comprehensive comparative analysis of machine learning classifiers for intrusion detection. We evaluated five distinct models on two real-world IoT network traffic datasets to identify the most effective algorithms for detecting malicious activity. Our findings show that tree-based models, specifically Random Forest and Decision Trees, deliver outstanding performance, achieving accuracies exceeding 99%. This research provides a clear benchmark and practical guidance for developing robust and high-performance security systems to protect vulnerable IoT environments.</p>
                </td>
              </tr>
              <tr bgcolor="#CBC3E3">
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Hybridnet.png" alt="mtabvqa" width="100%"></td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <p></p>
                  <a href="data/paper_hybridnet_preprint.pdf"><strong>HybridNet: LLM-Guided Active Learning for Multimodal Fake News Detection</strong></a><br>
                  Shreyas Kumar Tah, Lucky Gupta, Prajeet Katari, <strong>Anshul Singh</strong> et al.<br>
                  <em>Preprint</em><br>
                  <p>We propose HybridNet, a data-efficient framework that leverages hybrid active learning to select the most informative samples, drastically reducing labeling cost. We also propose a lightweight Reasoning-Aware Classifier (RAC) for challenging cases, which combines Vision–Language Model (VLM) features with reasoning from a Multimodal Large Language Model (MLLM) to further improve detection performance and provide human- interpretable explanations.</p>
                </td>
              </tr>
            </tbody>
          </table>

     

          <!-- Reflections / Projects -->
          <section>
            <h2 class="section-heading">Reflections</h2>
            <div class="project-container">
              <div class="project-grid">
                <div class="project-card">
                  <img src="Blogs/project2/dic.jpeg" alt="DIC" class="project-logo">
                  <div class="project-title">My DIC Journey</div>
                  <div class="project-links"><a href="Blogs/dic-research.html" target="_blank">thoughts</a></div>
                </div>
                <div class="project-card">
                  <img src="images/IITR.svg" alt="IITR" class="project-logo">
                  <div class="project-title">Research at IIT</div>
                  <div class="project-links"><a href="Blogs/iit-research.html" target="_blank">thoughts</a></div>
                </div>
              </div>
            </div>
          </section>

        </td>
      </tr>
    </tbody>
  </table>

  <!-- Footer -->
  <div id="footer">
    <div id="clustrmaps-widget">
      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=ODPcbudMEnNNGCw42b6i2skAv6rmQ2zrbfq00FLAg44&co=242628&cmn=9e3fdb&ct=edeaea'></script>
    </div>
    <p>&copy; Anshul Singh | Last update: Sep. 08, 2025</p>
  </div>
</body>
</html>
