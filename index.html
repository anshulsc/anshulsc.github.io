<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Anshul Singh</title>
  <meta name="author" content="Anshul Singh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/tornot.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <style>
    html {
      scroll-behavior: smooth;
    }

    body {
      margin: 0;
      background: #fafafa;
      color: #222;
      line-height: 1.5;
    }

    table {
      width: 100%;
      max-width: 1140px;
      margin: 0 auto;
      border: 0;
      border-spacing: 0;
    }

    td {
      padding: 0;
    }

    name {
      font-size: 2.2rem;
      font-weight: 700;
    }

    a {
      color: #6e46be;
      text-decoration: none;
    }

    a:hover {
      color: #b088f9;
      text-decoration: underline;
    }

    h2 {
      margin-top: 40px;
      margin-bottom: 20px;
      font-size: 1.8rem;
      text-align: center;
    }

    /* --- Reflections (Projects) --- */
    .section-heading {
      font-size: 2rem;
      margin-bottom: 30px;
      color: #111;
    }

    .project-container {
      max-width: 1140px;
      margin: 0 auto;
      padding: 0 20px;
    }

    .project-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 25px;
    }

    .project-card {
      background: #fff;
      border: 1px solid #e0e0e0;
      border-radius: 12px;
      padding: 25px 20px;
      text-align: center;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }

    .project-card:hover {
      transform: translateY(-6px);
      box-shadow: 0 8px 20px rgba(0, 0, 0, 0.08);
    }

    .project-logo {
      width: 100%;
      height: 140px;
      object-fit: contain;
      margin-bottom: 15px;
    }

    .project-title {
      font-size: 1.2rem;
      font-weight: 600;
      margin-bottom: 10px;
    }

    .project-links a {
      font-size: 1rem;
      color: #8b60e1;
      font-weight: 500;
    }

    /* --- Footer --- */
    #footer {
      margin-top: 60px;
      padding: 30px 0;
      text-align: center;
      font-size: 0.9rem;
      color: #555;
    }

    #clustrmaps-widget {
      margin: 10px auto 20px;
      max-width: 300px;
    }

    /* --- Responsive tweaks --- */
    @media (max-width: 768px) {
      name {
        font-size: 1.8rem;
      }

      .project-grid {
        grid-template-columns: 1fr;
      }

      #footer {
        padding: 20px 10px;
      }
    }
  </style>
</head>

<body>
  <table>
    <tbody>
      <tr>
        <td>
          <table>
            <tbody>
              <tr>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Anshul Singh</name>
                  </p>
                  <p>I am a <strong>Research Associate at the <a href="https://www.iisc.ac.in/">IACV Lab, IISc
                        Bangalore</a></strong>, advised by <strong><a
                        href="https://scholar.google.co.in/citations?user=Rscf2pYAAAAJ&hl=en">Prof. Soma
                        Biswas</a></strong>. My research centers on Large Language Models, with a particular focus on
                    multimodal systems, visual reasoning, and document understanding. My work explores how to enhance
                    the reasoning capabilities of language and vision models. Specifically, I have worked on:</p>
                  <ul>
                    <li>Adversarial Robustness and Safety of <strong>Multimodal LLMs </strong> and <strong>Diffusion
                        Models</strong></li>
                    <li>Multimodal Analysis of open source information using LLM-guided <strong>Active Learning</strong>
                    </li>
                    <li>Multimodal reasoning for multi-tabular data using Vision-Language Models</li>
                    <li><strong>Multilingual alignment</strong> for low-resource Indic languages</li>
                    <li>Vision Transformers and Signal analysis for fault detection and vocal pattern recognition</li>
                  </ul>
                  <p>Previously, I was a research intern at the <strong>LT Research Group, <a
                        href="https://www.uni-hamburg.de/">University of Hamburg</a></strong>, with <a
                      href="https://www.inf.uni-hamburg.de/en/inst/ab/lt/people/chris-biemann.html">Prof. Chris
                      Biemann</a> and Jan Strich, and a <strong><a
                        href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">MITACS
                        Globalink Research Intern</a></strong> at <a href="https://www.dal.ca/">Dalhousie
                      University</a>. I completed my Bachelor of Engineering in Information Technology at <a
                      href="https://puchd.ac.in/">Panjab University</a>. During my undergraduate studies, I also worked
                    as a machine learning research intern at <a href="https://www.iitr.ac.in/">IIT Roorkee</a> and
                    served as the ML/AI Lead at Google Developer Student Clubs.</p>
                  <p><strong>Research Interests:</strong> Multimodal and Multilingual Reasoning | Vision-Language Models
                    | Interpretability of LLMs | Information Retrieval</p>
                  <p><strong>Hobbies:</strong> Outside of research, I enjoy traveling, reading, writing blogs,
                    minimalist living, and tinkering with small experiments. I'm always happy to connect for a
                    discussion or collaboration!</p>
                  <p style="text-align:center">[
                    <a href="mailto:anshulsinghchambial@gmail.com" class="social-icon"><i
                        class="fa fa-envelope"></i></a> &nbsp/&nbsp
                    <a href="data/my_CV.pdf" class="social-icon"><i class="ai ai-cv"></i></a> &nbsp/&nbsp
                    <a href="https://github.com/anshulsc" class="social-icon"><i class="fa fa-github"></i></a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/anshulsc" class="social-icon"><i class="fa fa-twitter"></i></a>
                    &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=5vI_nOwAAAAJ" class="social-icon"><i
                        class="ai ai-google-scholar"></i></a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/anshulsc/" class="social-icon"><i
                        class="fa fa-linkedin"></i></a> &nbsp/&nbsp
                    <a href="Blogs/blog-index.html" class="social-icon"><i class="fa fa-rss"></i></a>]
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="gallery.html" title="Gallery"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/prof3.png"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <h2>Experience & Education</h2>
          <table class="about-edu" width="100%">
            <tr>
              <td align="center" width="14%"><a href="https://iisc.ac.in/"><img src="images/iisc.jpg" alt="IISc"
                    width="50%"></a></td>
              <td align="center" width="14%"><a href="https://www.uni-hamburg.de/en.html"><img src="images/UHH.svg"
                    alt="UHamburg" width="50%"></a></td>
              <td align="center" width="14%"><a href="https://www.dal.ca/"><img src="images/dal.jpeg" alt="Dalhousie"
                    width="50%"></a></td>
            </tr>

            <tr>
              <td align="center" style="padding-bottom: 40px;">Research Associate<br><b>IISc, Bangalore</b><br>Aug 2025
                â€“ Present</td>
              <td align="center" style="padding-bottom: 40px;">Research Intern<br><b>LT Group, UHH</b><br>Jan 2025 â€“ May
                2025</td>
              <td align="center" style="padding-bottom: 40px;">Undergrad Researcher<br><b>Dalhousie University,
                  Halifax</b><br>Oct 2024 - April 2025</td>
            </tr>

            <tr>
              <td align="center" width="14%"><a href="https://www.mitacs.ca/"><img src="images/mitacs.jpeg" alt="mitacs"
                    width="50%"></a></td>
              <td align="center" width="14%"><a href="https://www.iitr.ac.in/"><img src="images/IITR.svg" alt="IITR"
                    width="50%"></a></td>
              <td align="center" width="14%"><a href="https://puchd.ac.in/"><img src="images/panjab.png" alt="PU"
                    width="50%"></a></td>
            </tr>

            <tr>
              <td align="center">Research Intern<br><b>MITACS, Canada</b><br>June 2024 â€“ Sep 2024</td>
              <td align="center">Machine Learning Research Intern<br><b>IIT, Roorkee</b><br>June 2023 â€“ July 2023</td>
              <td align="center">B.E Information Technology<br><b>Panjab University</b><br>Sep 2021 â€“ June 2025</td>
            </tr>
          </table>


          <h2>News</h2>
          <div class="scrollable-news" style="max-height:400px;overflow-y:auto;padding-right:8px;">
            <ul class="timeline"
              style="list-style:none;padding:0;margin:0;display:flex;flex-direction:column;gap:10px;">
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>Nov 2025</b>:</div>
                <div><b>Paper</b> - Our paper <a
                    href="https://openreview.net/forum?id=20wKerGfSD&referrer=%5Bthe%20profile%20of%20Anshul%20Singh%5D(%2Fprofile%3Fid%3D~Anshul_Singh1)#discussion">Lost
                    in Translation and Noise: A Deep Dive into Failure-Mode of VLLMs on Real-World Tables</a> is
                  accepted at EurIPS Workshop on AI for Tabular Data. <b>(Oral ðŸŽ‰)</b></div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>Aug 2025</b>:</div>
                <div><b>Paper</b> - Our paper <a href="https://arxiv.org/abs/2506.11684">MTabVQA: Evaluating
                    Multi-Tabular Reasoning of Language Models in Visual Space</a> is accepted at EMNLP 2025 (Findings).
                </div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>Aug 2025</b>:</div>
                <div><b>Position</b> - Started working as Research Associate at <a
                    href="https://sites.google.com/iisc.ac.in/somabiswas/iacv-lab-iisc">IACV Lab</a>, IISc Bangalore.
                </div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>May 2025</b>:</div>
                <div><b>Paper</b> - Our new preprint, <a href="https://arxiv.org/abs/2506.11684">MTabVQA: Evaluating
                    Multi-Tabular Reasoning of Language Models in Visual Space</a>, is now available on ArXiv.</div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>Jan 2025</b>:</div>
                <div><b>Position</b> - Started a research internship at <a
                    href="https://www.inf.uni-hamburg.de/en/inst/ab/lt/home.html">Language Technology Group</a>,
                  University of Hamburg, Germany.</div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>Oct 2024</b>:</div>
                <div><b>Position</b> - Started working as undergraduate researcher at <a
                    href="https://web.cs.dal.ca/~tushar/smart/">SMART Lab</a>, Dalhousie University.</div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>July 2024</b>:</div>
                <div><b>Position</b> - Selected for <a
                    href="https://www.mitacs.ca/our-programs/globalink-research-internship-students/">Mitacs Globalink
                    Research Internship</a> at Dalhousie University, Nova Scotia, Canada.</div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>Nov 2023</b>:</div>
                <div><b>Paper</b> - Our work <a href="https://ieeexplore.ieee.org/document/10306428">Comparative
                    Analysis of State-of-the-Art Attack Detection Models</a> was accepted at 14th International
                  Conference on Computing Communication and Networking Technologies (ICCCNT).</div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>June 2023</b>:</div>
                <div><b>Position</b> - Started a Machine Learning Research Intern at <a
                    href="https://www.iitr.ac.in/">Virtual Labs, IIT Roorkee</a>.</div>
              </li>
              <li style="display:flex;gap:12px;">
                <div style="white-space:nowrap;"><b>March 2022</b>:</div>
                <div><b>Position</b> - Started working as Project Intern at Design & Innovation Centre, Panjab
                  University.</div>
              </li>
            </ul>
          </div>

          <h2>Research</h2>
          <table style="width:100%;border:0;border-spacing:0;">
            <tbody>
              <!-- <tr bgcolor="#cac4db">
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/mtabvqa.png" alt="mtabvqa"
                    width="100%"></td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://supernova-event.ai/"><strong>Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables</strong></a><br>
                  <strong>Anshul Singh</strong>, Rohan Chaudhary, Gagneet Singh, Abhay Kumar<br>
                  <em>Empirical Methods of Natural Language Processing (EMNLP), 2025 Findings</em><br>
                  <a href="https://openreview.net/forum?id=20wKerGfSD#discussion">Paper</a> / <a
                    href="https://huggingface.co/datasets/anshulsc/MirageTVQA">Dataset</a> 
                  <p>In this work, we address a critical gap in Vision-Language Model (VLM) evaluation by introducing
                    MTabVQA, a novel benchmark for multi-tabular visual question answering. Our benchmark comprises
                    3,745 complex question-answer pairs that require multi-hop reasoning across several visually
                    rendered table images, simulating real-world documents. We benchmark state-of-the-art VLMs,
                    revealing significant limitations in their ability to reason over complex visual data. To address
                    this, we release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments
                    demonstrate that fine-tuning with our dataset substantially improves VLM performance, bridging the
                    gap between existing benchmarks that rely on single or non-visual tables.</p>
                </td>
              </tr> -->
              <tr bgcolor="#cac4db">
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/mtabvqa.png" alt="mtabvqa"
                    width="100%"></td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://supernova-event.ai/"><strong>MTabVQA: Evaluating Multi-Tabular Reasoning of Language
                      Models in Visual Space</strong></a><br>
                  <strong>Anshul Singh</strong>, <a
                    href="https://www.inf.uni-hamburg.de/en/inst/ab/lt/people/chris-biemann.html">Chris Biemann</a>, Jan
                  Strich<br>
                  <em>Empirical Methods of Natural Language Processing (EMNLP), 2025 Findings</em><br>
                  <a href="https://arxiv.org/abs/2506.11684">Paper</a> / <a
                    href="https://huggingface.co/mtabvqa">Dataset</a> / <a
                    href="data/poster_mtabqa_emnlp.pdf">Poster</a>
                  <p>In this work, we address a critical gap in Vision-Language Model (VLM) evaluation by introducing
                    MTabVQA, a novel benchmark for multi-tabular visual question answering. Our benchmark comprises
                    3,745 complex question-answer pairs that require multi-hop reasoning across several visually
                    rendered table images, simulating real-world documents. We benchmark state-of-the-art VLMs,
                    revealing significant limitations in their ability to reason over complex visual data. To address
                    this, we release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments
                    demonstrate that fine-tuning with our dataset substantially improves VLM performance, bridging the
                    gap between existing benchmarks that rely on single or non-visual tables.</p>
                </td>
              </tr>
              <tr bgcolor="#cac4db">
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cot.jpeg"
                    alt="IoT Intrusion Detection Analysis" width="100%"></td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://doi.org/10.1109/ICCCNT56998.2023.10306428"><strong>Comparative Analysis of
                      State-of-the-Art Attack Detection Models</strong></a><br>
                  Priyanka Kumari, Veenu Mangat, and <strong>Anshul Singh</strong><br>
                  <em>14th International Conference on Computing Communication and Networking Technologies (ICCCNT),
                    2023</em><br>
                  <a href="https://doi.org/10.1109/ICCCNT56998.2023.10306428">Paper</a>
                  <p>In this work, we address the growing security challenges in IoT networks by conducting a
                    comprehensive comparative analysis of machine learning classifiers for intrusion detection. We
                    evaluated five distinct models on two real-world IoT network traffic datasets to identify the most
                    effective algorithms for detecting malicious activity. Our findings show that tree-based models,
                    specifically Random Forest and Decision Trees, deliver outstanding performance, achieving accuracies
                    exceeding 99%. This research provides a clear benchmark and practical guidance for developing robust
                    and high-performance security systems to protect vulnerable IoT environments.</p>
                </td>
              </tr>
              <tr bgcolor="#cac4db">
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Hybridnet.png" alt="mtabvqa"
                    width="100%"></td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <p></p>
                  <a href="data/paper_hybridnet_preprint.pdf"><strong>HybridNet: LLM-Guided Active Learning for
                      Multimodal Fake News Detection</strong></a><br>
                  Shreyas Kumar Tah, Lucky Gupta, Prajeet Katari, <strong>Anshul Singh</strong> et al.<br>
                  <em>Preprint</em><br>
                  <p>We propose HybridNet, a data-efficient framework that leverages hybrid active learning to select
                    the most informative samples, drastically reducing labeling cost. We also propose a lightweight
                    Reasoning-Aware Classifier (RAC) for challenging cases, which combines Visionâ€“Language Model (VLM)
                    features with reasoning from a Multimodal Large Language Model (MLLM) to further improve detection
                    performance and provide human- interpretable explanations.</p>
                </td>
              </tr>
            </tbody>
          </table>



          <!-- Reflections / Projects -->
          <section>
            <h2>Reflections</h2>
            <div class="project-container">
              <div class="project-grid">
                <div class="project-card">
                  <img src="Blogs/project2/dic.jpeg" alt="DIC" class="project-logo">
                  <div class="project-title">My DIC Journey</div>
                  <div class="project-links"><a href="Blogs/dic-research.html" target="_blank">thoughts</a></div>
                </div>
                <div class="project-card">
                  <img src="images/IITR.svg" alt="IITR" class="project-logo">
                  <div class="project-title">Research at IIT</div>
                  <div class="project-links"><a href="Blogs/iit-research.html" target="_blank">thoughts</a></div>
                </div>
              </div>
            </div>
          </section>

        </td>
      </tr>
    </tbody>
  </table>

  <!-- Footer -->
  <div id="footer">
    <div id="clustrmaps-widget">
      <script type='text/javascript' id='clustrmaps'
        src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=ODPcbudMEnNNGCw42b6i2skAv6rmQ2zrbfq00FLAg44&co=242628&cmn=9e3fdb&ct=edeaea'></script>
    </div>
    <p>&copy; Anshul Singh | Last update: Dec. 03, 2025</p>
  </div>
</body>

</html>